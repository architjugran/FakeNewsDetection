{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from feature_engineering import refuting_features, polarity_features, hand_features, gen_or_load_feats\n",
    "from feature_engineering import word_overlap_features\n",
    "from Dataset import DataSet\n",
    "from generate_test_splits import kfold_split, get_stances_for_folds\n",
    "from score import report_score, LABELS, score_submission\n",
    "from simpletransformers.classification import (\n",
    "    ClassificationModel, ClassificationArgs\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "number_folds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1683"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = DataSet()\n",
    "len(d.articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds,hold_out = kfold_split(d,training=1.0,n_folds=2)\n",
    "fold_stances, hold_out_stances = get_stances_for_folds(d,folds,hold_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Headline': \"Christian Bale passes on role of Steve Jobs, actor reportedly felt he wasn't right for part\", 'Body ID': 137, 'Stance': 'unrelated'}\n"
     ]
    }
   ],
   "source": [
    "print(fold_stances[0][0])\n",
    "#LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "lookup = dict()\n",
    "lookup['agree'] = 0\n",
    "lookup['disagree'] = 1\n",
    "lookup['discuss'] = 0\n",
    "lookup['unrelated'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for fold_id in range(len(fold_stances)):\n",
    "    xdd = []\n",
    "    for i in range(len(fold_stances[fold_id])):\n",
    "        article_text = d.articles[fold_stances[fold_id][i]['Body ID']]\n",
    "        new_list = [fold_stances[fold_id][i]['Headline'] , article_text,int(lookup[fold_stances[fold_id][i]['Stance']])]\n",
    "        xdd.append(new_list)\n",
    "    train_data.append(xdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Christian Bale passes on role of Steve Jobs, actor reportedly felt he wasn't right for part\",\n",
       " '30-year-old Moscow resident was hospitalized with wounds very intimate nature. As it became known LifeNews, in the hands of doctors, the man complained that his casual acquaintance opoila in the sauna, and then gently held his castration operation. And actions criminals were executed with surgical precision - woman sewed all the smallest blood vessels.\\n\\n\\n\\n- I met a girl at the bar, and then we went to the sauna for a taxi. I remember that I had a beer, and more do not remember, - said Roman law enforcement officials.\\n\\nAt the hospital, the man hours spent in intensive care, doctors were forced to spend a second operation on the testicles.\\n\\n- I thought I had just cut, but at the hospital they told me that I removed the testicles - told Roman about his sad adventures.\\n\\n- The patient had literally jewelry operation - he cut out eggs and ligated vessels. We can say that the transactions were either physician or veterinarian - doctors say.\\n\\nInterestingly, the wife of the injured man is still convinced that her husband was in the hospital because of surgery on the genitals caused by a serious illness.',\n",
       " 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text_a  \\\n",
      "0      Hundreds of Palestinians flee floods in Gaza a...   \n",
      "1      HBO and Apple in Talks for $15/Month Apple TV ...   \n",
      "2      Spider burrowed through tourist's stomach and ...   \n",
      "3      Accused Boston Marathon Bomber Severely Injure...   \n",
      "4          British Aid Worker Confirmed Murdered By ISIS   \n",
      "...                                                  ...   \n",
      "19782  Nicaraguan commission on possible meteorite cr...   \n",
      "19783     Would-be rapist has penis severed by angry mob   \n",
      "19784  'Banksy' Reacts To Paris Attack With Poignant ...   \n",
      "19785  Indian 'Rapist' Castrated By Angry Mob Who Hac...   \n",
      "19786  Brian Williams slams social media for speculat...   \n",
      "\n",
      "                                                  text_b  labels  \n",
      "0      Hundreds of Palestinians were evacuated from t...       0  \n",
      "1      (Reuters) - A Canadian soldier was shot at the...       1  \n",
      "2      Fear not arachnophobes, the story of Bunbury's...       1  \n",
      "3      A British fighter who travelled to Iraq to sto...       1  \n",
      "4      The British Islamic State militant who has fea...       1  \n",
      "...                                                  ...     ...  \n",
      "19782  CHILPANCINGO, Mexico (AP) — Authorities testin...       1  \n",
      "19783  The London Metropolitan Police has denied repo...       1  \n",
      "19784  Police in Carlisle are hunting a middle-aged m...       1  \n",
      "19785  Money makes the world go round, right?\\n\\nLunc...       1  \n",
      "19786  Along with unveiling the Apple Watch earlier t...       1  \n",
      "\n",
      "[19787 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3eb1328e9e64696a42c5c332bc502f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20563.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a93098c648344bfb8070b9614e3dfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c2386544c54539b1c9d62f8bb7cbe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running Epoch 0 of 1', max=2571.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-32637716eba4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0meval_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m                     \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m                     \u001b[0;31m# torch.nn.utils.clip_grad_norm_(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0;31m#     model.parameters(), args.max_grad_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iters in range(len(fold_stances)):\n",
    "    train_df = pd.DataFrame(train_data[iters])\n",
    "    train_df.columns = [\"text_a\", \"text_b\", \"labels\"]\n",
    "    \n",
    "    eval_df = pd.DataFrame(train_data[1-iters])\n",
    "    eval_df.columns = [\"text_a\", \"text_b\", \"labels\"]\n",
    "    eval_labels = eval_df[\"labels\"].tolist()\n",
    "    eval_df.drop(\"labels\" , axis = 1)\n",
    "    #print(train_df)\n",
    "    print(eval_df)\n",
    "    model_args = ClassificationArgs(num_train_epochs=1)\n",
    "    model = ClassificationModel(\"roberta\", \"roberta-base\",use_cuda=False)\n",
    "    \n",
    "    # Train the model\n",
    "    model.train_model(train_df)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    result, model_outputs, wrong_predictions = model.eval_model(eval_df)\n",
    "    \n",
    "    predicted = [LABELS[int(a)] for a in model_outputs]\n",
    "    actual = [LABELS[int(a)] for a in eval_labels]\n",
    "\n",
    "    fold_score, _ = score_submission(actual, predicted)\n",
    "    max_fold_score, _ = score_submission(actual, actual)\n",
    "\n",
    "    score = fold_score/max_fold_score\n",
    "\n",
    "    print(\"Score for fold \"+ str(iters) + \" was - \" + str(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
